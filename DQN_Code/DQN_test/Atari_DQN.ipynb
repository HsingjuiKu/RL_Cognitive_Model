{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haydengu/anaconda3/envs/rl-summer/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py:246: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool8: (False, True),\n",
      "/Users/haydengu/anaconda3/envs/rl-summer/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Lambda\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from collections import deque\n",
    "import random\n",
    "import cv2\n",
    "import stable_baselines3.common.atari_wrappers as atari_wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import atari roms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python -m atari_py.import_roms ROMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version +a7a216c)\n",
      "[Powered by Stella]\n",
      "/Users/haydengu/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gym/utils/seeding.py:159: DeprecationWarning: \u001B[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001B[0m\n",
      "  deprecation(\n",
      "/Users/haydengu/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gym/utils/seeding.py:203: DeprecationWarning: \u001B[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001B[0m\n",
      "  deprecation(\n",
      "/Users/haydengu/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/haydengu/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "#Initialize environment. v4 means no action repeat\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "\n",
    "#wraps env with these preprocessing options:\n",
    "#values will be scaled at training time to save memory\n",
    "\"\"\"Atari 2600 preprocessings. \n",
    "    This class follows the guidelines in \n",
    "    Machado et al. (2018), \"Revisiting the Arcade Learning Environment: \n",
    "    Evaluation Protocols and Open Problems for General Agents\".\n",
    "    Specifically:\n",
    "    * NoopReset: obtain initial state by taking random number of no-ops on reset. \n",
    "    * Frame skipping: 4 by default\n",
    "    * Max-pooling: most recent two observations\n",
    "    * Termination signal when a life is lost: turned off by default. Not recommended by Machado et al. (2018).\n",
    "    * Resize to a square image: 84x84 by default\n",
    "    * Grayscale observation: optional\n",
    "    * Scale observation: optional\"\"\"\n",
    "env = gym.wrappers.AtariPreprocessing(env, noop_max=30, frame_skip=4, \n",
    "                                      screen_size=84, terminal_on_life_loss=False, \n",
    "                                      grayscale_obs=True, grayscale_newaxis=False, scale_obs=False)\n",
    "\n",
    "env = gym.wrappers.FrameStack(env, 4)\n",
    "\n",
    "env = atari_wrappers.ClipRewardEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#actions in this environment\n",
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of frames to run\n",
    "NUM_FRAMES = 1000000\n",
    "\n",
    "#number of episodes to run\n",
    "NUM_EPISODES = 50\n",
    "\n",
    "#max iterations per run\n",
    "MAX_ITERATIONS = 1000000\n",
    "\n",
    "#epsilon for choosing action\n",
    "eps = 1\n",
    "\n",
    "#minimum eps\n",
    "eps_min = 0.1\n",
    "\n",
    "#eps linear decay for first 10% of run\n",
    "eps_linear_decay = (eps-eps_min)/(NUM_FRAMES/5)\n",
    "\n",
    "#discount factor for future utility\n",
    "discount_factor = 0.99\n",
    "\n",
    "#batch size for exp replay\n",
    "batch_size = 32\n",
    "\n",
    "#max memory stored for exp replay\n",
    "MAX_MEMORY = int(NUM_FRAMES/10)\n",
    "\n",
    "#initial population of memory using random policy\n",
    "INIT_MEMORY = int(NUM_FRAMES/20)\n",
    "\n",
    "#update interval to use target network\n",
    "TARGET_C = int(NUM_FRAMES/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/6n9cccss24g7s1y2w9tj4kvm0000gn/T/ipykernel_22560/3055028092.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for _ in tqdm_notebook(range(1)):\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1bbaffb1747840bda3d3d66014553f9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haydengu/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001B[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001B[0m\n",
      "  logger.warn(\n",
      "/Users/haydengu/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001B[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001B[0m\n",
      "  logger.warn(\n",
      "/Users/haydengu/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001B[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reset() got an unexpected keyword argument 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 9\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m#iterate through 10 playthroughs\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m tqdm_notebook(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m)):\n\u001B[1;32m      7\u001B[0m     \n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m#reset env\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m     \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m     done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     score \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gymnasium/core.py:414\u001B[0m, in \u001B[0;36mWrapper.reset\u001B[0;34m(self, seed, options)\u001B[0m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\n\u001B[1;32m    411\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m, seed: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, options: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    412\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[1;32m    413\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 414\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gym/wrappers/frame_stack.py:197\u001B[0m, in \u001B[0;36mFrameStack.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     obs, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mreset(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 197\u001B[0m     obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    198\u001B[0m     info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# Unused\u001B[39;00m\n\u001B[1;32m    199\u001B[0m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframes\u001B[38;5;241m.\u001B[39mappend(obs) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_stack)]\n",
      "File \u001B[0;32m~/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gym/wrappers/atari_preprocessing.py:157\u001B[0m, in \u001B[0;36mAtariPreprocessing.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    155\u001B[0m     _, reset_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mreset(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 157\u001B[0m     _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    158\u001B[0m     reset_info \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    160\u001B[0m noops \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    161\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39munwrapped\u001B[38;5;241m.\u001B[39mnp_random\u001B[38;5;241m.\u001B[39mintegers(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnoop_max \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnoop_max \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    164\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gym/wrappers/time_limit.py:86\u001B[0m, in \u001B[0;36mTimeLimit.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001B[39;00m\n\u001B[1;32m     78\u001B[0m \n\u001B[1;32m     79\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;124;03m    The reset environment\u001B[39;00m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:42\u001B[0m, in \u001B[0;36mOrderEnforcing.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 42\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gym/core.py:415\u001B[0m, in \u001B[0;36mWrapper.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    413\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[ObsType, Tuple[ObsType, \u001B[38;5;28mdict\u001B[39m]]:\n\u001B[1;32m    414\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Resets the environment with kwargs.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 415\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gym/wrappers/env_checker.py:45\u001B[0m, in \u001B[0;36mPassiveEnvChecker.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_reset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_reset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 45\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43menv_reset_passive_checker\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mreset(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/rl-summer/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:200\u001B[0m, in \u001B[0;36menv_reset_passive_checker\u001B[0;34m(env, **kwargs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    196\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFuture gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    197\u001B[0m     )\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# Checks the result of env.reset with kwargs\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn_info\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m    203\u001B[0m         result, \u001B[38;5;28mtuple\u001B[39m\n\u001B[1;32m    204\u001B[0m     ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe result returned by `env.reset(return_info=True)` was not a tuple, actual type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(result)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mTypeError\u001B[0m: reset() got an unexpected keyword argument 'seed'"
     ]
    }
   ],
   "source": [
    "#keep scores\n",
    "scores = []\n",
    "frames = 0\n",
    "\n",
    "#iterate through 10 playthroughs\n",
    "for _ in tqdm_notebook(range(1)):\n",
    "    \n",
    "    #reset env\n",
    "    env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    #while game is not over\n",
    "    while not done:\n",
    "        #render env\n",
    "        env.render()\n",
    "        frames += 1\n",
    "        \n",
    "        #execute random action\n",
    "        _, reward, done, _ = env.step(env.action_space.sample())\n",
    "        \n",
    "        #track score\n",
    "        score += reward\n",
    "        \n",
    "    #append to score list\n",
    "    scores.append(score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with image input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(eps, model, env, state):\n",
    "    ''' Returns an action using epsilon greedy strategy\n",
    "    Args:\n",
    "    - eps (int): chance for random action\n",
    "    - model (Model): Keras model used to choose best action\n",
    "    - env (EnvSpec): Gym environment\n",
    "    \n",
    "    Returns:\n",
    "    - (int): index of best action\n",
    "    '''\n",
    "    #exploration\n",
    "    if np.random.random() < eps:\n",
    "        #exploration\n",
    "        action = np.random.randint(0, env.action_space.n)\n",
    "        return action\n",
    "    else:\n",
    "        #exploitation\n",
    "        #use expand_dims here to add a dimension for input layer\n",
    "        q_vals = model.predict(state)\n",
    "        action = np.argmax(q_vals)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_replay(memory, model, target_model, discount_factor, batch_size):\n",
    "    ''' Fits the model with minibatch of states from memory\n",
    "    Args:\n",
    "    - memory (Array): array of environment transitions\n",
    "    - model (Model): Keras model to be fit\n",
    "    - target_model (Model): Keras model to get target Q val\n",
    "    - discount_factor (float): discount factor for future utility\n",
    "    - batch_size (int): size of minibatch\n",
    "    \n",
    "    Returns: None\n",
    "    '''\n",
    "    \n",
    "    #if memory is less than batch size, return nothing\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    else:\n",
    "        states = []\n",
    "        targets = []\n",
    "        \n",
    "        #sample a batch\n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "        \n",
    "        #iterate through bastch\n",
    "        for state, action, reward, new_state, done in minibatch:\n",
    "            #scale states to be [0,1]. We only scale before fitting cuz storing uint8 is cheaper\n",
    "            state = state/255\n",
    "            new_state = new_state/255\n",
    "\n",
    "            target = reward\n",
    "            \n",
    "            #if game not over, target q val includes discounted future utility\n",
    "            #we use a cloned model to predict here for stability. Model is changed every C frames\n",
    "            #we use the online model to choose best action to deal with overestimation error (Double-Q learning)\n",
    "            if not done:\n",
    "                best_future_action = np.argmax(model.predict(new_state))\n",
    "                target = reward + discount_factor * target_model.predict(new_state)[0][best_future_action]\n",
    "            \n",
    "            #get current actions vector\n",
    "            target_vector = model.predict(state)[0]\n",
    "            \n",
    "            #update current action q val with target q val\n",
    "            target_vector[action] = target\n",
    "            \n",
    "            #add to states\n",
    "            states.append(state)\n",
    "            \n",
    "            #add to targets\n",
    "            targets.append(target_vector)\n",
    "            \n",
    "        #fit model\n",
    "        model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncwh_to_nwhc(tensor):\n",
    "    '''Converts tensor from NCWH to NWHC\n",
    "    Args:\n",
    "    - tensor (4D Array): NCWH tensor\n",
    "    \n",
    "    Returns:\n",
    "    - (4D Array): tensor in NWHC format\n",
    "    '''\n",
    "    return tf.transpose(tensor, [0, 2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I use lambda layer so I can convert NCWH to NWHC since CPU training doesn't support NCWH\n",
    "model = Sequential(\n",
    "    [\n",
    "        Lambda(ncwh_to_nwhc, output_shape=(84, 84, 4), input_shape=(4, 84, 84)),\n",
    "        Conv2D(16, kernel_size=(8, 8), strides=4, activation=\"relu\", input_shape=(4, 84, 84)),\n",
    "        Conv2D(32, kernel_size=(4, 4), strides=2, activation=\"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dense(env.action_space.n, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rms = tf.keras.optimizers.RMSprop(learning_rate=0.00025, momentum=0.95, epsilon=0.01)\n",
    "model.compile(loss=tf.keras.losses.Huber(), optimizer=rms)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = Sequential(\n",
    "    [\n",
    "        Lambda(ncwh_to_nwhc, output_shape=(84, 84, 4), input_shape=(4, 84, 84)),\n",
    "        Conv2D(16, kernel_size=(8, 8), strides=4, activation=\"relu\", input_shape=(4, 84, 84)),\n",
    "        Conv2D(32, kernel_size=(4, 4), strides=2, activation=\"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dense(env.action_space.n, activation=\"linear\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prefill memory with INIT_MEMORY frames\n",
    "\n",
    "#init memory using deque to only store MAX_MEMORY\n",
    "memory = deque(maxlen=MAX_MEMORY)\n",
    "\n",
    "#progress bar\n",
    "pbar = tqdm_notebook(total=INIT_MEMORY)\n",
    "\n",
    "#playthrough game until memory is prefilled\n",
    "while len(memory) < INIT_MEMORY:\n",
    "    \n",
    "    #reset env\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    \n",
    "    #playthrough\n",
    "    while not done:\n",
    "        \n",
    "        #random action\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        #execute action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #add transition to memory\n",
    "        memory.append([np.expand_dims(state, axis=0), action, reward, np.expand_dims(new_state, axis=0), done])\n",
    "        \n",
    "        #progress bar\n",
    "        pbar.update(1)\n",
    "        \n",
    "        #update state\n",
    "        state = new_state\n",
    "        \n",
    "#close progress bar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init scores\n",
    "scores = []\n",
    "\n",
    "#init total_frames\n",
    "total_frames = 0\n",
    "\n",
    "#init num_updates\n",
    "num_updates = 0\n",
    "\n",
    "#init fitness history\n",
    "fit_hist = {'loss': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbar = tqdm_notebook(total=50)\n",
    "\n",
    "#run frames\n",
    "while total_frames < NUM_FRAMES:\n",
    "        \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    frames = 0\n",
    "            \n",
    "    #playing through this round\n",
    "    for frame in range(MAX_ITERATIONS):\n",
    "        env.render()\n",
    "        \n",
    "        frames += 1\n",
    "        \n",
    "        #epsilon greedy choose action\n",
    "        action = epsilon_greedy(eps, model, env, np.expand_dims(state, axis=0))\n",
    "        \n",
    "        \n",
    "        #execute action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #track score\n",
    "        score += reward\n",
    "        \n",
    "        #memorize\n",
    "        memory.append([np.expand_dims(state, axis=0), action, reward, np.expand_dims(new_state, axis=0), done])\n",
    "        \n",
    "        #exp replay\n",
    "        experience_replay(memory, model, model, discount_factor, batch_size)\n",
    "        \n",
    "        #clone target network every C frames\n",
    "        num_updates += batch_size\n",
    "        \n",
    "        if num_updates > TARGET_C:\n",
    "            num_updates = 0\n",
    "            target_model.set_weights(model.get_weights())\n",
    "            \n",
    "            #save memory and model\n",
    "            np.save('memory', memory)\n",
    "            model.save('tmp_model')\n",
    "            \n",
    "        \n",
    "        #update state\n",
    "        state = new_state\n",
    "        \n",
    "        #decay epsilon\n",
    "        eps -= eps_linear_decay\n",
    "        eps = max(eps, eps_min)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    scores.append(score)\n",
    "    total_frames += frames\n",
    "    pbar.update(1)\n",
    "    \n",
    "pbar.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "plt.plot(scores)\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('episodes')\n",
    "plt.title('CartPole')\n",
    "\n",
    "reg = LinearRegression().fit(np.arange(len(scores)).reshape(-1, 1), np.array(scores).reshape(-1, 1))\n",
    "y_pred = reg.predict(np.arange(len(scores)).reshape(-1, 1))\n",
    "plt.plot(y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying policy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "score = 0\n",
    "state = env.reset()\n",
    "q_hist = []\n",
    "scores = []\n",
    "\n",
    "for _ in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        state = np.array(state)/255\n",
    "        action = np.argmax(model.predict(np.expand_dims(state, axis=0)))\n",
    "        q_hist.append(model.predict(np.expand_dims(state, axis=0)).mean())\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        state = new_state\n",
    "    scores.append(score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('tmp_model', custom_objects={'tf':tf})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.expand_dims(env.reset(), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "728c223486afcc150a80a1652d40dbe61283ae708253ba05ab4e921fdaff8aab"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
