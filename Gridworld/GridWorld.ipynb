{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-08T00:57:50.025861Z",
     "start_time": "2024-05-08T00:57:49.810301Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, grid_size=3, gamma=0.9, penalty=-12, reward=10, goal_reward=5, max_steps=5):\n",
    "        self.grid_size = grid_size\n",
    "        self.gamma = gamma\n",
    "        self.penalty = penalty\n",
    "        self.reward = reward\n",
    "        self.goal_reward = goal_reward\n",
    "        self.max_steps = max_steps\n",
    "        self.state_visits = np.zeros((grid_size, grid_size))\n",
    "        self.reset()\n",
    "        self.start = (0, 0)  # Define the start position\n",
    "\n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.steps = 0\n",
    "        self.state_visits[:] = 0\n",
    "        return (self.x, self.y)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.state_visits[self.x, self.y] += 1\n",
    "\n",
    "        if action == 0 and self.y != 0:\n",
    "            self.y -= 1\n",
    "        elif action == 1 and self.x != self.grid_size-1:\n",
    "            self.x += 1\n",
    "        elif action == 2 and self.y != self.grid_size-1:\n",
    "            self.y += 1\n",
    "        elif action == 3 and self.x != 0:\n",
    "            self.x -= 1\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        if self.x == self.grid_size - 1 and self.y == self.grid_size - 1:\n",
    "            return (self.x, self.y), self.goal_reward, True\n",
    "        elif self.steps >= self.max_steps:\n",
    "            return (self.x, self.y), 0, True\n",
    "        else:\n",
    "            return (self.x, self.y), np.random.choice([self.penalty, self.reward], p=[0.5, 0.5]), False\n",
    "\n",
    "    def get_epsilon(self, state):\n",
    "        x, y = state\n",
    "        n_s = self.state_visits[x, y]\n",
    "        return 1 / np.sqrt(n_s + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "def Q_learning(env, num_episodes, max_steps):\n",
    "    Q_table = np.zeros((env.grid_size, env.grid_size, 4))\n",
    "    N_table = np.zeros((env.grid_size, env.grid_size, 4))  # Keep track of the counts\n",
    "    rewards = np.zeros(num_episodes)\n",
    "    max_action_values = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        # alpha = 1 / (episode + 1)  # Decaying learning rate\n",
    "        while not done and steps < max_steps:\n",
    "            # epsilon = env.get_epsilon(state)\n",
    "            epsilon = 1 / np.sqrt(episode + 1)  # Decaying epsilon-greedy\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = np.random.randint(0, 4)  # Explore\n",
    "            else:\n",
    "                action = np.argmax(Q_table[state])  # Exploit\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            N_table[state][action] += 1  # Increase the count\n",
    "            alpha = 1 / N_table[state][action]  # Decaying learning rate based on the count\n",
    "            target = reward + env.gamma * np.max(Q_table[next_state])\n",
    "            Q_table[state][action] = Q_table[state][action] + alpha * (target - Q_table[state][action])\n",
    "            state = next_state\n",
    "        rewards[episode] = total_reward\n",
    "        max_action_values.append(np.max(Q_table[env.start]))\n",
    "    average_rewards = np.cumsum(rewards) / (np.arange(num_episodes) + 1)\n",
    "    return average_rewards, max_action_values\n",
    "\n",
    "def double_Q_learning(env, num_episodes, max_steps):\n",
    "    Q1_table = np.zeros((env.grid_size, env.grid_size, 4))\n",
    "    Q2_table = np.zeros((env.grid_size, env.grid_size, 4))\n",
    "    N_table = np.zeros((env.grid_size, env.grid_size, 4))  # Keep track of the counts\n",
    "    rewards = np.zeros(num_episodes)\n",
    "    max_action_values = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            # epsilon = env.get_epsilon(state)\n",
    "            epsilon = 1 / np.sqrt(episode + 1)  # Decaying epsilon\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = np.random.randint(0, 4)  # Explore\n",
    "            else:\n",
    "                action = np.argmax(Q1_table[state] + Q2_table[state])  # Exploit\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            N_table[state][action] += 1  # Increase the count\n",
    "            alpha = 1 / N_table[state][action]  # Decaying learning rate based on the count\n",
    "            if np.random.uniform(0, 1) < 0.5:\n",
    "                target = reward + env.gamma * Q2_table[next_state][np.argmax(Q1_table[next_state])]\n",
    "                Q1_table[state][action] = Q1_table[state][action] + alpha * (target - Q1_table[state][action])\n",
    "            else:\n",
    "                target = reward + env.gamma * Q1_table[next_state][np.argmax(Q2_table[next_state])]\n",
    "                Q2_table[state][action] = Q2_table[state][action] + alpha * (target - Q2_table[state][action])\n",
    "            state = next_state\n",
    "        rewards[episode] = total_reward\n",
    "        max_action_values.append(np.max(Q1_table[env.start] + Q2_table[env.start]))\n",
    "    average_rewards = np.cumsum(rewards) / (np.arange(num_episodes) + 1)\n",
    "    return average_rewards, max_action_values\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def clipped_max(Q_values, tau):\n",
    "    A = len(Q_values)\n",
    "    qt = np.ones(A) * tau / (A - 1)\n",
    "    qt[np.argmax(Q_values)] = 1 - tau\n",
    "    return qt\n",
    "\n",
    "def clipped_softmax(Q_values, beta):\n",
    "    sorted_indices = np.argsort(Q_values)[-3:]\n",
    "    clipped_Q_values = np.full(Q_values.shape, -np.inf)\n",
    "    clipped_Q_values[sorted_indices] = Q_values[sorted_indices]\n",
    "    e_x = np.exp(beta * clipped_Q_values)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def smoothed_Q_learning(env, num_episodes, max_steps, smoothing_strategy=\"softmax\"):\n",
    "    Q_table = np.zeros((env.grid_size, env.grid_size, 4))\n",
    "    N_table = np.zeros((env.grid_size, env.grid_size, 4))  # Keep track of the counts\n",
    "    rewards = np.zeros(num_episodes)\n",
    "    max_action_values = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        beta = 0.1 + 0.1 * episode  # Decaying beta\n",
    "        tau = np.exp(-0.02 * episode)  # Decaying tau\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            epsilon = 1 / np.sqrt(episode + 1)  # Decaying epsilon\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = np.random.randint(0, 4)  # Explore\n",
    "            else:\n",
    "            #     Big Q Action Selection\n",
    "            #     action = np.argmax(Q_table[state])  # Exploit\n",
    "            #     Small Q Action Selection\n",
    "                if smoothing_strategy == \"softmax\":\n",
    "                    qt = softmax(Q_table[state])\n",
    "                elif smoothing_strategy == \"clipped_softmax\":\n",
    "                    qt = clipped_softmax(Q_table[state], beta)\n",
    "                elif smoothing_strategy == \"clipped_max\":\n",
    "                    qt = clipped_max(Q_table[state], tau)\n",
    "\n",
    "                action = np.argmax(qt * Q_table[state])  # Exploit based on max q-value\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            N_table[state][action] += 1  # Increase the count\n",
    "            alpha = 1 / N_table[state][action]  # Decaying learning rate based on the count\n",
    "\n",
    "            if smoothing_strategy == \"softmax\":\n",
    "                qt = softmax(Q_table[next_state])\n",
    "            elif smoothing_strategy == \"clipped_softmax\":\n",
    "                qt = clipped_softmax(Q_table[next_state], beta)\n",
    "            elif smoothing_strategy == \"clipped_max\":\n",
    "                qt = clipped_max(Q_table[next_state], tau)\n",
    "\n",
    "            target = reward + env.gamma * np.sum(qt * Q_table[next_state])\n",
    "            Q_table[state][action] = Q_table[state][action] + alpha * (target - Q_table[state][action])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        rewards[episode] = total_reward\n",
    "        max_action_values.append(np.max(Q_table[env.start]))\n",
    "\n",
    "    average_rewards = np.cumsum(rewards) / (np.arange(num_episodes) + 1)\n",
    "\n",
    "    return average_rewards, max_action_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T00:57:50.033398Z",
     "start_time": "2024-05-08T00:57:50.031445Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# class QAttentionTransformer(nn.Module):\n",
    "#     def __init__(self, state_dim, action_dim, embed_dim):\n",
    "#         super(QAttentionTransformer, self).__init__()\n",
    "#         self.state_embed = nn.Linear(state_dim, embed_dim)\n",
    "#         self.query_embed = nn.Parameter(torch.randn(embed_dim))\n",
    "#         self.key_embed = nn.Linear(action_dim, embed_dim)\n",
    "#         self.value_embed = nn.Linear(action_dim, embed_dim)\n",
    "#         self.scale = torch.sqrt(torch.tensor(embed_dim, dtype=torch.float32))\n",
    "# \n",
    "#     def forward(self, state, q_values):\n",
    "#         # Embeddings\n",
    "#         state_emb = self.state_embed(state)\n",
    "#         query = self.query_embed.unsqueeze(0).repeat(state.size(0), 1)\n",
    "#         key = self.key_embed(q_values)\n",
    "#         value = self.value_embed(q_values)\n",
    "# \n",
    "#         # Attention mechanism modified by Q-values\n",
    "#         attention_scores = torch.bmm(query.unsqueeze(1), key.unsqueeze(2)) / self.scale  # [batch_size, 1, 1]\n",
    "#         attention_scores = attention_scores.squeeze(2) + q_values\n",
    "#         attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "#         \n",
    "#         # Weighted sum of values\n",
    "#         weighted_values = torch.bmm(attention_weights.unsqueeze(1), value.unsqueeze(2)).squeeze(2)  # [batch_size, embed_dim]\n",
    "#         return attention_weights, weighted_values\n",
    "# \n",
    "# \n",
    "# def smoothed_Q_learning_with_attention(env, num_episodes, max_steps, model):\n",
    "#     Q_table = np.zeros((env.grid_size, env.grid_size, 4))\n",
    "#     N_table = np.zeros((env.grid_size, env.grid_size, 4))\n",
    "#     rewards = np.zeros(num_episodes)\n",
    "# \n",
    "#     for episode in range(num_episodes):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "#         steps = 0\n",
    "#         while not done and steps < max_steps:\n",
    "#             state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "# \n",
    "#             q_values = Q_table[state]\n",
    "#             action = np.random.choice(4)  # 从0到3中随机选择一个动作\n",
    "#             q_values_tensor = torch.tensor(Q_table[state[0], state[1], :], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "# \n",
    "#             action_probs, _ = model(state_tensor.unsqueeze(0), q_values_tensor)\n",
    "#             action_probs = action_probs.squeeze().cpu().numpy()\n",
    "# \n",
    "#             if np.random.rand() < 1 / np.sqrt(episode + 1):\n",
    "#                 action = np.random.randint(0, 4)\n",
    "#             else:\n",
    "#                 action = np.random.choice(4, p=action_probs)\n",
    "# \n",
    "#             next_state, reward, done = env.step(action)\n",
    "#             total_reward += reward\n",
    "#             steps += 1\n",
    "# \n",
    "#             N_table[state][action] += 1\n",
    "#             alpha = 1 / N_table[state][action]\n",
    "#             qt = np.dot(action_probs, Q_table[next_state])\n",
    "# \n",
    "#             target = reward + env.gamma * qt\n",
    "#             Q_table[state][action] = (1 - alpha) * Q_table[state][action] + alpha * target\n",
    "# \n",
    "#             state = next_state\n",
    "# \n",
    "#         rewards[episode] = total_reward\n",
    "#         max_action_values.append(np.max(Q_table[env.start]))\n",
    "#     average_rewards = np.cumsum(rewards) / (np.arange(num_episodes) + 1)\n",
    "# \n",
    "#     return average_rewards, max_action_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T00:57:50.039244Z",
     "start_time": "2024-05-08T00:57:50.034619Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Prior Mean (mu_prior)\n",
    "In Gridworld, the agent receives a mixture of penalties and rewards as it navigates through the grid, with a special goal reward for reaching the terminal state. Since the agent randomly receives either a penalty or a reward with equal probability at non-terminal states and considering the impact of the goal reward, the prior mean should reflect an average outcome of these interactions. Given the penalty of -12, a reward of 10, and a goal reward of 5, but considering that the goal reward is only received once at the end, a slightly pessimistic or neutral prior mean might be appropriate. For instance, considering the mixed nature of outcomes, a mu_prior around -1 or 0 might initially set a neutral expectation, reflecting uncertainty about whether penalties or rewards will dominate the agent's experience.\n",
    "\n",
    "Prior Variance (sigma_prior)\n",
    "The variance should capture the uncertainty or variability in the agent's expectations about rewards. Given the stark contrast between penalties and rewards in this environment, a relatively high variance would be appropriate to capture the wide range of possible outcomes the agent can expect. A sigma_prior of around 1 to 5 could be suitable, indicating significant uncertainty in initial reward expectations.\n",
    "\n",
    "Observation Noise (sigma_observation)\n",
    "The observation noise reflects the variance or \"noise\" in the observed rewards, apart from the agent's inherent uncertainty. In this Gridworld setup, where the agent has a 50/50 chance to receive either a penalty or a reward at non-terminal states, the variability in observed rewards can be considered as part of the environment's inherent randomness. Since the rewards and penalties are distinct and deterministic once the outcome of the random choice is known, the observation noise could be relatively low. However, to account for the randomness in selecting between a penalty and a reward, a sigma_observation of around 0.1 to 1 could acknowledge this aspect without overly emphasizing it.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "mu_prior = -1\n",
    "sigma_prior = 3\n",
    "sigma_observation = 0.5\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def update_bayesian_posterior(mu_prior, sigma_prior, rewards, sigma_observation=0.5):\n",
    "    n = len(rewards)\n",
    "    reward_sum = np.sum(rewards)\n",
    "    sigma_posterior_squared = 1 / (1 / sigma_prior**2 + n / sigma_observation**2)\n",
    "    mu_posterior = (mu_prior / sigma_prior**2 + reward_sum / sigma_observation**2) * sigma_posterior_squared\n",
    "    return mu_posterior, np.sqrt(sigma_posterior_squared)\n",
    "\n",
    "def smoothed_Q_learning_with_Cognitive_model(env, num_episodes, max_steps, mu_prior=-1, sigma_prior=3):\n",
    "    Q_table = np.zeros((env.grid_size, env.grid_size, 4))\n",
    "    N_table = np.zeros((env.grid_size, env.grid_size, 4))\n",
    "    rewards = np.zeros(num_episodes)\n",
    "    max_action_values = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        episode_rewards = []  # Collect rewards for Bayesian update\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            epsilon = 1 / np.sqrt(episode + 1)  # Decaying epsilon\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = np.random.randint(0, 4)  # Explore\n",
    "            else:\n",
    "                # # Exploit: consider Bayesian update here if applying before action selection\n",
    "                # action = np.argmax(Q_table[state])  # Exploit\n",
    "                adjusted_Q_values = Q_table[state] + mu_prior  # assuming mu_prior is the Bayesian-updated expectation\n",
    "                action = np.argmax(adjusted_Q_values)\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            N_table[state][action] += 1\n",
    "            alpha = 1 / N_table[state][action]  # Learning rate based on count\n",
    "\n",
    "            # Bayesian update\n",
    "            mu_prior, sigma_prior = update_bayesian_posterior(mu_prior, sigma_prior, episode_rewards, sigma_observation)\n",
    "\n",
    "            # Use updated mu_prior to adjust Q values\n",
    "            adjusted_Q = Q_table[next_state] + mu_prior\n",
    "            qt = softmax(adjusted_Q)  # Apply softmax on adjusted Q values for smoothing\n",
    "\n",
    "            target = reward + env.gamma * np.dot(qt, adjusted_Q)  # Use dot product for expected return\n",
    "            Q_table[state][action] = Q_table[state][action] + alpha * (target - Q_table[state][action])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        rewards[episode] = total_reward\n",
    "        max_action_values.append(np.max(Q_table[env.start]))\n",
    "\n",
    "    average_rewards = np.cumsum(rewards) / (np.arange(num_episodes) + 1)\n",
    "\n",
    "    return average_rewards, max_action_values\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T00:57:50.045857Z",
     "start_time": "2024-05-08T00:57:50.042086Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "num_experiments = 5000\n",
    "num_episodes = 10000\n",
    "max_steps = 5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T00:57:50.046138Z",
     "start_time": "2024-05-08T00:57:50.044172Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "Q_rewards = []\n",
    "double_Q_rewards = []\n",
    "Q_max_action_values = []\n",
    "double_Q_max_action_values = []\n",
    "smoothed_Q_softmax_rewards = []\n",
    "smoothed_Q_softmax_max_action_values = []\n",
    "smoothed_Q_clipped_max_rewards = []\n",
    "smoothed_Q_clipped_max_max_action_values = []\n",
    "smoothed_Q_clipped_softmax_rewards = []\n",
    "smoothed_Q_clipped_softmax_max_action_values = []\n",
    "smoothed_Q_congitive_model_rewards = []\n",
    "smoothed_Q_congitive_model_max_action_values = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T00:57:50.052195Z",
     "start_time": "2024-05-08T00:57:50.046932Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Learning Completed\n"
     ]
    }
   ],
   "source": [
    "for _ in range(num_experiments):\n",
    "    env = GridWorld()\n",
    "    rewards, max_action_values = Q_learning(env, num_episodes, max_steps)\n",
    "    Q_rewards.append(rewards)\n",
    "    Q_max_action_values.append(max_action_values)\n",
    "\n",
    "print(\"Q Learning Completed\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T02:51:56.267436Z",
     "start_time": "2024-05-08T00:57:50.048286Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double Q Learning Completed\n"
     ]
    }
   ],
   "source": [
    "for _ in range(num_experiments):\n",
    "    env = GridWorld()\n",
    "    rewards, max_action_values = double_Q_learning(env, num_episodes, max_steps)\n",
    "    double_Q_rewards.append(rewards)\n",
    "    double_Q_max_action_values.append(max_action_values)\n",
    "\n",
    "print(\"Double Q Learning Completed\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T07:01:23.361874Z",
     "start_time": "2024-05-08T02:51:56.266034Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in range(num_experiments):\n",
    "    env = GridWorld()\n",
    "    rewards, max_action_values = smoothed_Q_learning(env, num_episodes, max_steps, smoothing_strategy=\"softmax\")\n",
    "    smoothed_Q_softmax_rewards.append(rewards)\n",
    "    smoothed_Q_softmax_max_action_values.append(max_action_values)\n",
    "\n",
    "print(\"Smoothed Q Learning Softmax Completed\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-08T07:01:23.414614Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in range(num_experiments):\n",
    "    env = GridWorld()\n",
    "    rewards, max_action_values = smoothed_Q_learning(env, num_episodes, max_steps, smoothing_strategy=\"clipped_max\")\n",
    "    smoothed_Q_clipped_max_rewards.append(rewards)\n",
    "    smoothed_Q_clipped_max_max_action_values.append(max_action_values)\n",
    "\n",
    "print(\"Smoothed Q Learning Clipped Max Completed\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in range(num_experiments):\n",
    "    env = GridWorld()\n",
    "    rewards, max_action_values = smoothed_Q_learning(env, num_episodes, max_steps, smoothing_strategy=\"clipped_softmax\")\n",
    "    smoothed_Q_clipped_softmax_rewards.append(rewards)\n",
    "    smoothed_Q_clipped_softmax_max_action_values.append(max_action_values)\n",
    "\n",
    "print(\"Smoothed Q Learning Clipped Softmax Completed\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in range(num_experiments):\n",
    "    env = GridWorld()\n",
    "    rewards, max_action_values = smoothed_Q_learning_with_Cognitive_model(env, num_episodes, max_steps)\n",
    "    smoothed_Q_congitive_model_rewards.append(rewards)\n",
    "    smoothed_Q_congitive_model_max_action_values.append(max_action_values)\n",
    "\n",
    "print(\"Smoothed Q Learning Bayesian Inference Completed\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# state_dim = 9\n",
    "# action_dim = 4\n",
    "# embed_dim = 128\n",
    "# \n",
    "# device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = QAttentionTransformer(state_dim, action_dim, embed_dim).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# \n",
    "# for _ in range(num_experiments):\n",
    "#     env = GridWorld()\n",
    "#     rewards, max_action_values = smoothed_Q_learning_with_attention(env, num_episodes, max_steps, model)\n",
    "#     smoothed_Q_congitive_model_rewards.append(rewards)\n",
    "#     smoothed_Q_congitive_model_max_action_values.append(max_action_values)\n",
    "# \n",
    "# print(\"Smoothed Q Learning Cognition Model Completed\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Q_rewards = np.mean(np.array(Q_rewards), axis=0)\n",
    "double_Q_rewards = np.mean(np.array(double_Q_rewards), axis=0)\n",
    "Q_max_action_values = np.mean(Q_max_action_values, axis=0)\n",
    "double_Q_max_action_values = np.mean(double_Q_max_action_values, axis=0)\n",
    "\n",
    "smoothed_Q_softmax_rewards = np.mean(np.array(smoothed_Q_softmax_rewards), axis=0)\n",
    "smoothed_Q_softmax_max_action_values = np.mean(smoothed_Q_softmax_max_action_values, axis=0)\n",
    "smoothed_Q_clipped_max_rewards = np.mean(np.array(smoothed_Q_clipped_max_rewards), axis=0)\n",
    "smoothed_Q_clipped_max_max_action_values = np.mean(smoothed_Q_clipped_max_max_action_values, axis=0)\n",
    "smoothed_Q_clipped_softmax_rewards = np.mean(np.array(smoothed_Q_clipped_softmax_rewards), axis=0)\n",
    "smoothed_Q_clipped_softmax_max_action_values = np.mean(smoothed_Q_clipped_softmax_max_action_values, axis=0)\n",
    "smoothed_Q_congitive_model_rewards = np.mean(np.array(smoothed_Q_congitive_model_rewards), axis=0)\n",
    "smoothed_Q_congitive_model_max_action_values = np.mean(np.array(smoothed_Q_congitive_model_max_action_values), axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(12, 16))\n",
    "\n",
    "axs[0].plot(Q_rewards, label=\"Q-learning\")\n",
    "axs[0].plot(double_Q_rewards, label=\"Double Q-learning\")\n",
    "axs[0].plot(smoothed_Q_softmax_rewards, label=\"Belief Q-learning (Softmax)\")\n",
    "axs[0].plot(smoothed_Q_clipped_max_rewards, label=\"Belief Q-learning (Clipped Max)\")\n",
    "axs[0].plot(smoothed_Q_clipped_softmax_rewards, label=\"Belief Q-learning (Clipped SoftMax)\")\n",
    "axs[0].plot(smoothed_Q_congitive_model_rewards, label=\"Belief Q-learning (Bayes Inference)\")\n",
    "axs[0].set_xlabel(\"Steps\")\n",
    "axs[0].set_ylabel(\"Average Reward\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "\n",
    "axs[1].plot(Q_max_action_values, label=\"Q-learning\")\n",
    "axs[1].plot(double_Q_max_action_values, label=\"Double Q-learning\")\n",
    "axs[1].plot(smoothed_Q_softmax_max_action_values, label=\"Belief Q-learning (Softmax)\")\n",
    "axs[1].plot(smoothed_Q_clipped_max_max_action_values, label=\"Belief Q-learning (Clipped Max)\")\n",
    "axs[1].plot(smoothed_Q_clipped_max_max_action_values, label=\"Belief Q-learning (Clipped Max)\")\n",
    "axs[1].plot(smoothed_Q_congitive_model_max_action_values, label=\"Belief Q-learning (Bayes Congitive Model)\")\n",
    "axs[1].set_xlabel(\"Steps\")\n",
    "axs[1].set_ylabel(\"Max Action Value at Start State\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
