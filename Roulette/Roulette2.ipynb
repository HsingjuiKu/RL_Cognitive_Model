{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqzUuJ1AaXsI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQEMPZXgaia9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535667a3-5cba-4e77-c0ed-a4ce8def9d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Define a Roulette environment class, inherited from gym.Env\n",
        "class RouletteEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(RouletteEnv, self).__init__()  # Inherited gym.Env\n",
        "        self.action_space = spaces.Discrete(171)  # Define a discrete action space with 171 possible actions\n",
        "        self.observation_space = spaces.Discrete(1)  # Define an observation space with 1 possible state\n",
        "        self.current_step = 0  # Initialize a counter for the current step\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 170:  # If the action is to stop playing\n",
        "            return 0, 0, True, {}\n",
        "\n",
        "        winning_number = np.random.choice(37)  # Choose a random winning number from 0 to 36\n",
        "        reward = -1  # Assume player bets $1 each time and initialize the reward as -1\n",
        "\n",
        "        # If player bets on a number and wins, they get a reward of 35\n",
        "        if action < 36 and action == winning_number:\n",
        "            reward = 35\n",
        "\n",
        "        # If player bets on red or black and wins, they get a reward of 1\n",
        "        elif action == 36 or action == 37:\n",
        "            if (action == 36 and winning_number % 2 == 1) or (action == 37 and winning_number % 2 == 0):\n",
        "                reward = 1\n",
        "\n",
        "        # Bet on even or odd\n",
        "        elif action == 38 or action == 39:\n",
        "            if (action == 38 and winning_number % 2 == 0) or (action == 39 and winning_number % 2 == 1):\n",
        "                reward = 1\n",
        "\n",
        "        # Bet on low (1-18) or high (19-36)\n",
        "        elif action == 40 or action == 41:\n",
        "            if (action == 40 and 1 <= winning_number <= 18) or (action == 41 and 19 <= winning_number <= 36):\n",
        "                reward = 1\n",
        "\n",
        "        # Bet on first 12, second 12, or third 12\n",
        "        elif action >= 42 and action <= 44:\n",
        "            if (action == 42 and 1 <= winning_number <= 12) or \\\n",
        "                    (action == 43 and 13 <= winning_number <= 24) or \\\n",
        "                    (action == 44 and 25 <= winning_number <= 36):\n",
        "                reward = 2\n",
        "\n",
        "        # Bet on column\n",
        "        elif action >= 45 and action <= 47:\n",
        "            if (action == 45 and winning_number % 3 == 1) or \\\n",
        "                    (action == 46 and winning_number % 3 == 2) or \\\n",
        "                    (action == 47 and winning_number % 3 == 0):\n",
        "                reward = 2\n",
        "\n",
        "        # Bet on two adjacent numbers\n",
        "        elif action >= 48 and action <= 82:\n",
        "            if winning_number in [(action - 48), (action - 47)]:\n",
        "                reward = 17\n",
        "\n",
        "        # Bet on two numbers at the column intersection\n",
        "        elif action >= 83 and action <= 94:\n",
        "            columns = [(0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36), (1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34),\n",
        "                       (2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35)]\n",
        "            column_pairs = [(columns[0], columns[1]), (columns[1], columns[2])]\n",
        "            for pair in column_pairs:\n",
        "                if winning_number in pair[0] and winning_number in pair[1]:\n",
        "                    reward = 17\n",
        "\n",
        "        return 0, reward, False, {}\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the environment to its initial state\n",
        "        self.current_step = 0\n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "# Define a function for selecting an action using epsilon-greedy strategy\n",
        "def epsilon_greedy(Q, state, epsilon):\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        action = np.random.choice(Q.shape[1])  # Randomly select an action\n",
        "    else:\n",
        "        action = np.argmax(Q[state])  # Select the action with the highest estimated reward\n",
        "    return action\n",
        "\n",
        "# Define a function for calculating a polynomial learning rate\n",
        "def polynomial_learning_rate(base, exponent, t):\n",
        "    t = max(t, 1)  # Ensure t is not zero\n",
        "    return base / (t ** exponent)\n",
        "\n",
        "def decay_epsilon(epsilon_start, epsilon_end, decay_rate, i_episode):\n",
        "    return epsilon_end + (epsilon_start - epsilon_end) * math.exp(-decay_rate * i_episode)\n",
        "\n",
        "\n",
        "# The Q_learning function implements the standard Q-learning algorithm.\n",
        "# It takes as arguments the environment, the number of episodes, the base learning rate, learning rate exponent, epsilon, and the discount factor gamma.\n",
        "\n",
        "def Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma):\n",
        "    Q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "    n_table = np.zeros_like(Q_table)\n",
        "    MaxQ_table = np.full([env.observation_space.n, env.action_space.n], 0)\n",
        "    abs_errors = []\n",
        "    avg_action_values = []\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        alpha = polynomial_learning_rate(base_learning_rate, learning_rate_exponent, i_episode)\n",
        "        epsilon = decay_epsilon(epsilon_start, epsilon_end, decay_rate, i_episode)\n",
        "        actions = []\n",
        "        while not done:\n",
        "            action = epsilon_greedy(Q_table, state, epsilon)\n",
        "            n_table[state, action] += 1\n",
        "            # alpha = 1 / n_table[state, action]\n",
        "            actions.append(action)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            old_value = Q_table[state][action]\n",
        "            Q_table[state][action] = (1 - alpha) * old_value + alpha * (reward + gamma * np.max(Q_table[next_state]))\n",
        "            state = next_state\n",
        "        abs_errors.append(np.mean(np.abs(Q_table - MaxQ_table)))\n",
        "        avg_action_values.append(np.mean(Q_table[state, :]))\n",
        "    return Q_table, abs_errors, avg_action_values\n",
        "\n",
        "\n",
        "def double_Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma):\n",
        "    Q1_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "    Q2_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "    n1_table = np.zeros_like(Q1_table)\n",
        "    n2_table = np.zeros_like(Q2_table)\n",
        "    MaxQ_table = np.full([env.observation_space.n, env.action_space.n], 0)\n",
        "    abs_errors = []\n",
        "    avg_action_values = []\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        alpha = polynomial_learning_rate(base_learning_rate, learning_rate_exponent, i_episode)\n",
        "        epsilon = decay_epsilon(epsilon_start, epsilon_end, decay_rate, i_episode)\n",
        "        actions = []\n",
        "        while not done:\n",
        "            action = epsilon_greedy(Q1_table + Q2_table, state, epsilon)\n",
        "            actions.append(action)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            if np.random.binomial(1, 0.5) == 1:\n",
        "                n1_table[state, action] += 1\n",
        "                # Paper Learning Rate\n",
        "                # alpha = 1 / n1_table[state, action]\n",
        "                best_next_action = np.argmax(Q1_table[next_state])\n",
        "                Q1_table[state][action] = (1 - alpha) * Q1_table[state][action] + alpha * (reward + gamma * Q2_table[next_state][best_next_action])\n",
        "            else:\n",
        "                n2_table[state, action] += 1\n",
        "                alpha = 1 / n2_table[state, action]\n",
        "                best_next_action = np.argmax(Q2_table[next_state])\n",
        "                Q2_table[state][action] = (1 - alpha) * Q2_table[state][action] + alpha * (reward + gamma * Q1_table[next_state][best_next_action])\n",
        "            state = next_state\n",
        "        abs_errors.append(np.mean(np.abs((Q1_table + Q2_table) / 2 - MaxQ_table)))\n",
        "        avg_action_values.append(np.mean((Q1_table + Q2_table)[state, :] / 2))\n",
        "    return Q1_table, Q2_table, abs_errors, avg_action_values\n",
        "\n",
        "def smoothed_Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma, selection_method):\n",
        "    Q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "    n_table = np.zeros_like(Q_table)\n",
        "    MaxQ_table = np.full([env.observation_space.n, env.action_space.n], 0)\n",
        "    abs_errors = []\n",
        "    avg_action_values = []\n",
        "\n",
        "    def softmax(Q_state, beta):\n",
        "        Q_normalized = (Q_state - np.mean(Q_state)) / (\n",
        "                    np.std(Q_state) + 1e-10)  # Added epsilon to avoid division by zero\n",
        "        exps = np.exp(beta * Q_normalized)\n",
        "        probs = exps / np.sum(exps)\n",
        "        probs = probs / np.sum(probs)  # Ensure probabilities sum to 1\n",
        "        if np.isnan(probs).any():\n",
        "            probs = np.ones_like(Q_state) / len(Q_state)\n",
        "        return probs\n",
        "\n",
        "    def clipped_softmax(Q_state, beta):\n",
        "        Q_clipped = np.ones_like(Q_state) * (-np.inf)\n",
        "        top_k_indices = np.argsort(Q_state)[-3:]\n",
        "        Q_clipped[top_k_indices] = Q_state[top_k_indices] - np.max(Q_state)  # Subtract the maximum value\n",
        "        exps = np.exp(beta * Q_clipped)\n",
        "        return exps / np.sum(exps)\n",
        "\n",
        "    def argmaxR(Q_state):\n",
        "        max_value_indices = np.argwhere(Q_state == np.amax(Q_state)).flatten().tolist()\n",
        "        return np.random.choice(max_value_indices)\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        beta = 0.1 + 0.1 * (i_episode + 1)\n",
        "        mu = np.exp(-0.02 * i_episode)\n",
        "        actions = []\n",
        "        epsilon = decay_epsilon(epsilon_start, epsilon_end, decay_rate, i_episode)\n",
        "        alpha = polynomial_learning_rate(base_learning_rate, learning_rate_exponent, i_episode)\n",
        "\n",
        "\n",
        "        while not done:\n",
        "            if np.random.rand() > epsilon:\n",
        "                action = argmaxR(Q_table[state])\n",
        "            else:\n",
        "                if selection_method == 'softmax':\n",
        "                    action = np.random.choice(np.arange(env.action_space.n), p=softmax(Q_table[state], beta))\n",
        "                elif selection_method == 'clipped_max':\n",
        "                    A = len(Q_table[state])\n",
        "                    action_distribution = np.full(A, mu / (A - 1))\n",
        "                    action_distribution[argmaxR(Q_table[state])] = 1 - mu\n",
        "                    action = np.random.choice(np.arange(env.action_space.n), p=action_distribution)\n",
        "                elif selection_method == 'clipped_softmax':\n",
        "                    action = np.random.choice(np.arange(env.action_space.n), p=clipped_softmax(Q_table[state], beta))\n",
        "\n",
        "            actions.append(action)\n",
        "            n_table[state, action] += 1\n",
        "            # alpha = 1 / n_table[state, action]\n",
        "\n",
        "            # Big Q Update\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if selection_method == 'softmax':\n",
        "                action_distribution = softmax(Q_table[next_state], beta)\n",
        "            elif selection_method == 'clipped_max':\n",
        "                A = len(Q_table[next_state])\n",
        "                action_distribution = np.ones(A) * mu / (A - 1) if A != 1 else np.ones(A)\n",
        "                as_ = argmaxR(Q_table[next_state])\n",
        "                action_distribution[as_] = 1 - mu\n",
        "            elif selection_method == 'clipped_softmax':\n",
        "                action_distribution = clipped_softmax(Q_table[next_state], beta)\n",
        "\n",
        "            expected_return = reward + gamma * np.sum(Q_table[next_state] * action_distribution)\n",
        "            Q_table[state][action] = Q_table[state][action] + alpha * (expected_return - Q_table[state][action])\n",
        "            state = next_state\n",
        "\n",
        "\n",
        "            # Small Q Update\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if selection_method == 'softmax':\n",
        "                action_distribution = softmax(Q_table[next_state], beta)\n",
        "            elif selection_method == 'clipped_max':\n",
        "                A = len(Q_table[next_state])\n",
        "                action_distribution = np.ones(A) * mu / (A - 1) if A != 1 else np.ones(A)\n",
        "                as_ = argmaxR(Q_table[next_state])\n",
        "                action_distribution[as_] = 1 - mu\n",
        "            elif selection_method == 'clipped_softmax':\n",
        "                action_distribution = clipped_softmax(Q_table[next_state], beta)\n",
        "\n",
        "            best_action = np.argmax(action_distribution) # select the action with the highest probability\n",
        "            expected_return = reward + gamma * Q_table[next_state][best_action] # use the Q value of the selected action only\n",
        "            Q_table[state][best_action] = Q_table[state][action] + alpha * (expected_return - Q_table[state][best_action])\n",
        "            state = next_state\n",
        "\n",
        "        abs_errors.append(np.mean(np.abs(Q_table - MaxQ_table)))\n",
        "        avg_action_values.append(np.mean(Q_table[state, :]))\n",
        "\n",
        "    return Q_table, abs_errors, avg_action_values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmvURpUzanA2"
      },
      "outputs": [],
      "source": [
        "# Create the environment\n",
        "env = RouletteEnv()\n",
        "\n",
        "# Set the parameters for the experiment\n",
        "num_episodes = 100000\n",
        "base_learning_rate = 0.3\n",
        "learning_rate_exponent = 0.5\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.1\n",
        "decay_rate = 0.0001\n",
        "gamma = 0.99\n",
        "delta_start = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQj8jGB2aq7S",
        "outputId": "426622f7-1452-4d69-e6f4-afaf848ab7be"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q Learning Completed!\n",
            "Double Q Learning Completed!\n"
          ]
        }
      ],
      "source": [
        "# Run the experiments\n",
        "_, Q_learning_abs_errors, Q_learning_avg_action_values = Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma)\n",
        "print(\"Q Learning Completed!\")\n",
        "_, _, double_Q_learning_abs_errors, double_Q_learning_avg_action_values = double_Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma)\n",
        "print(\"Double Q Learning Completed!\")\n",
        "_, softmax_smoothed_abs_errors, softmax_smoothed_avg_action_values = smoothed_Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma, \"softmax\")\n",
        "print(\"Softmax Smooth Q Learning Completed!\")\n",
        "_, clipped_max_smoothed_abs_errors, clipped_max_smoothed_avg_action_values = smoothed_Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma, \"clipped_max\")\n",
        "print(\"Clipped max Smooth Q Learning Completed!\")\n",
        "_, clipped_softmax_smoothed_abs_errors, clipped_softmax_smoothed_avg_action_values = smoothed_Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma, \"clipped_softmax\")\n",
        "print(\"Clipped Softmax Smooth Q Learning Completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}