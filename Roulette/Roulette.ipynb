{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QqzUuJ1AaXsI",
    "ExecuteTime": {
     "end_time": "2024-05-07T01:11:51.931262Z",
     "start_time": "2024-05-07T01:11:51.598740Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EQEMPZXgaia9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "535667a3-5cba-4e77-c0ed-a4ce8def9d73",
    "ExecuteTime": {
     "end_time": "2024-05-07T01:11:51.940138Z",
     "start_time": "2024-05-07T01:11:51.938418Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the Roulette Environment Reward Mechanism\n",
    "Define Epsilon Greedy Strategy\n",
    "Define Polynomial Learning Rate\n",
    "\"\"\"\n",
    "# Define a Roulette environment class, inherited from gym.Env\n",
    "class RouletteEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(RouletteEnv, self).__init__()  # Inherited gym.Env\n",
    "        self.action_space = spaces.Discrete(171)  # Define a discrete action space with 171 possible actions\n",
    "        self.observation_space = spaces.Discrete(1)  # Define an observation space with 1 possible state\n",
    "        self.current_step = 0  # Initialize a counter for the current step\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 170:  # If the action is to stop playing\n",
    "            return 0, 0, True, {}\n",
    "\n",
    "        winning_number = np.random.choice(37)  # Choose a random winning number from 0 to 36\n",
    "        reward = -1  # Assume player bets $1 each time and initialize the reward as -1\n",
    "\n",
    "        # If player bets on a number and wins, they get a reward of 35\n",
    "        if action < 36 and action == winning_number:\n",
    "            reward = 35\n",
    "\n",
    "        # If player bets on red or black and wins, they get a reward of 1\n",
    "        elif action == 36 or action == 37:\n",
    "            if (action == 36 and winning_number % 2 == 1) or (action == 37 and winning_number % 2 == 0):\n",
    "                reward = 1\n",
    "\n",
    "        # Bet on even or odd\n",
    "        elif action == 38 or action == 39:\n",
    "            if (action == 38 and winning_number % 2 == 0) or (action == 39 and winning_number % 2 == 1):\n",
    "                reward = 1\n",
    "\n",
    "        # Bet on low (1-18) or high (19-36)\n",
    "        elif action == 40 or action == 41:\n",
    "            if (action == 40 and 1 <= winning_number <= 18) or (action == 41 and 19 <= winning_number <= 36):\n",
    "                reward = 1\n",
    "\n",
    "        # Bet on first 12, second 12, or third 12\n",
    "        elif action >= 42 and action <= 44:\n",
    "            if (action == 42 and 1 <= winning_number <= 12) or \\\n",
    "                    (action == 43 and 13 <= winning_number <= 24) or \\\n",
    "                    (action == 44 and 25 <= winning_number <= 36):\n",
    "                reward = 2\n",
    "\n",
    "        # Bet on column\n",
    "        elif action >= 45 and action <= 47:\n",
    "            if (action == 45 and winning_number % 3 == 1) or \\\n",
    "                    (action == 46 and winning_number % 3 == 2) or \\\n",
    "                    (action == 47 and winning_number % 3 == 0):\n",
    "                reward = 2\n",
    "\n",
    "        # Bet on two adjacent numbers\n",
    "        elif action >= 48 and action <= 82:\n",
    "            if winning_number in [(action - 48), (action - 47)]:\n",
    "                reward = 17\n",
    "\n",
    "        # Bet on two numbers at the column intersection\n",
    "        elif action >= 83 and action <= 94:\n",
    "            columns = [(0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36), (1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34),\n",
    "                       (2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35)]\n",
    "            column_pairs = [(columns[0], columns[1]), (columns[1], columns[2])]\n",
    "            for pair in column_pairs:\n",
    "                if winning_number in pair[0] and winning_number in pair[1]:\n",
    "                    reward = 17\n",
    "\n",
    "        return 0, reward, False, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to its initial state\n",
    "        self.current_step = 0\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "# Define a function for selecting an action using epsilon-greedy strategy\n",
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = np.random.choice(Q.shape[1])  # Randomly select an action\n",
    "    else:\n",
    "        action = np.argmax(Q[state])  # Select the action with the highest estimated reward\n",
    "    return action\n",
    "\n",
    "# Define a function for calculating a polynomial learning rate\n",
    "def polynomial_learning_rate(base, exponent, t):\n",
    "    t = max(t, 1)  # Ensure t is not zero\n",
    "    return base / (t ** exponent)\n",
    "\n",
    "def decay_epsilon(epsilon_start, epsilon_end, decay_rate, i_episode):\n",
    "    return epsilon_end + (epsilon_start - epsilon_end) * math.exp(-decay_rate * i_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Q Learning Algorithm\n",
    "\"\"\"\n",
    "def Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma):\n",
    "    Q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    n_table = np.zeros_like(Q_table)\n",
    "    MaxQ_table = np.full([env.observation_space.n, env.action_space.n], 0)\n",
    "    abs_errors = []\n",
    "    avg_action_values = []\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        alpha = polynomial_learning_rate(base_learning_rate, learning_rate_exponent, i_episode)\n",
    "        epsilon = decay_epsilon(epsilon_start, epsilon_end, decay_rate, i_episode)\n",
    "        actions = []\n",
    "        while not done:\n",
    "            action = epsilon_greedy(Q_table, state, epsilon)\n",
    "            n_table[state, action] += 1\n",
    "            # alpha = 1 / n_table[state, action]\n",
    "            actions.append(action)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            old_value = Q_table[state][action]\n",
    "            Q_table[state][action] = (1 - alpha) * old_value + alpha * (reward + gamma * np.max(Q_table[next_state]))\n",
    "            state = next_state\n",
    "        abs_errors.append(np.mean(np.abs(Q_table - MaxQ_table)))\n",
    "        avg_action_values.append(np.mean(Q_table[state, :]))\n",
    "    return Q_table, abs_errors, avg_action_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T01:11:51.945148Z",
     "start_time": "2024-05-07T01:11:51.942950Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Double Q Learning Algorithm\n",
    "\"\"\"\n",
    "def double_Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma):\n",
    "    Q1_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    Q2_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    n1_table = np.zeros_like(Q1_table)\n",
    "    n2_table = np.zeros_like(Q2_table)\n",
    "    MaxQ_table = np.full([env.observation_space.n, env.action_space.n], 0)\n",
    "    abs_errors = []\n",
    "    avg_action_values = []\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        alpha = polynomial_learning_rate(base_learning_rate, learning_rate_exponent, i_episode)\n",
    "        epsilon = decay_epsilon(epsilon_start, epsilon_end, decay_rate, i_episode)\n",
    "        actions = []\n",
    "        while not done:\n",
    "            action = epsilon_greedy(Q1_table + Q2_table, state, epsilon)\n",
    "            actions.append(action)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            if np.random.binomial(1, 0.5) == 1:\n",
    "                n1_table[state, action] += 1\n",
    "                # alpha = 1 / n1_table[state, action]\n",
    "                best_next_action = np.argmax(Q1_table[next_state])\n",
    "                Q1_table[state][action] = (1 - alpha) * Q1_table[state][action] + alpha * (reward + gamma * Q2_table[next_state][best_next_action])\n",
    "            else:\n",
    "                n2_table[state, action] += 1\n",
    "                alpha = 1 / n2_table[state, action]\n",
    "                best_next_action = np.argmax(Q2_table[next_state])\n",
    "                Q2_table[state][action] = (1 - alpha) * Q2_table[state][action] + alpha * (reward + gamma * Q1_table[next_state][best_next_action])\n",
    "            state = next_state\n",
    "        abs_errors.append(np.mean(np.abs((Q1_table + Q2_table) / 2 - MaxQ_table)))\n",
    "        avg_action_values.append(np.mean((Q1_table + Q2_table)[state, :] / 2))\n",
    "    return Q1_table, Q2_table, abs_errors, avg_action_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T01:11:51.950046Z",
     "start_time": "2024-05-07T01:11:51.948117Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Smoothed Q Learning Algorithm\n",
    "\"\"\"\n",
    "def smoothed_Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma, selection_method):\n",
    "    Q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    n_table = np.zeros_like(Q_table)\n",
    "    MaxQ_table = np.full([env.observation_space.n, env.action_space.n], 0)\n",
    "    abs_errors = []\n",
    "    avg_action_values = []\n",
    "\n",
    "    def softmax(Q_state, beta):\n",
    "        Q_normalized = (Q_state - np.mean(Q_state)) / (\n",
    "                    np.std(Q_state) + 1e-10)  # Added epsilon to avoid division by zero\n",
    "        exps = np.exp(beta * Q_normalized)\n",
    "        probs = exps / np.sum(exps)\n",
    "        probs = probs / np.sum(probs)  # Ensure probabilities sum to 1\n",
    "        if np.isnan(probs).any():\n",
    "            probs = np.ones_like(Q_state) / len(Q_state)\n",
    "        return probs\n",
    "\n",
    "    def clipped_softmax(Q_state, beta):\n",
    "        Q_clipped = np.ones_like(Q_state) * (-np.inf)\n",
    "        top_k_indices = np.argsort(Q_state)[-3:]\n",
    "        Q_clipped[top_k_indices] = Q_state[top_k_indices] - np.max(Q_state)  # Subtract the maximum value\n",
    "        exps = np.exp(beta * Q_clipped)\n",
    "        return exps / np.sum(exps)\n",
    "\n",
    "    def argmaxR(Q_state):\n",
    "        max_value_indices = np.argwhere(Q_state == np.amax(Q_state)).flatten().tolist()\n",
    "        return np.random.choice(max_value_indices)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        beta = 0.1 + 0.1 * (i_episode + 1)\n",
    "        mu = np.exp(-0.02 * i_episode)\n",
    "        actions = []\n",
    "        epsilon = decay_epsilon(epsilon_start, epsilon_end, decay_rate, i_episode)\n",
    "        alpha = polynomial_learning_rate(base_learning_rate, learning_rate_exponent, i_episode)\n",
    "\n",
    "        while not done:\n",
    "            if np.random.rand() > epsilon:\n",
    "                action = argmaxR(Q_table[state])\n",
    "            else:\n",
    "                if selection_method == 'softmax':\n",
    "                    action = np.random.choice(np.arange(env.action_space.n), p=softmax(Q_table[state], beta))\n",
    "                elif selection_method == 'clipped_max':\n",
    "                    A = len(Q_table[state])\n",
    "                    action_distribution = np.full(A, mu / (A - 1))\n",
    "                    action_distribution[argmaxR(Q_table[state])] = 1 - mu\n",
    "                    action = np.random.choice(np.arange(env.action_space.n), p=action_distribution)\n",
    "                elif selection_method == 'clipped_softmax':\n",
    "                    action = np.random.choice(np.arange(env.action_space.n), p=clipped_softmax(Q_table[state], beta))\n",
    "            actions.append(action)\n",
    "            n_table[state, action] += 1\n",
    "            # alpha = 1 / n_table[state, action]\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            if selection_method == 'softmax':\n",
    "                action_distribution = softmax(Q_table[next_state], beta)\n",
    "            elif selection_method == 'clipped_max':\n",
    "                A = len(Q_table[next_state])\n",
    "                action_distribution = np.ones(A) * mu / (A - 1) if A != 1 else np.ones(A)\n",
    "                as_ = argmaxR(Q_table[next_state])\n",
    "                action_distribution[as_] = 1 - mu\n",
    "            elif selection_method == 'clipped_softmax':\n",
    "                action_distribution = clipped_softmax(Q_table[next_state], beta)\n",
    "\n",
    "            expected_return = reward + gamma * np.sum(Q_table[next_state] * action_distribution)\n",
    "            Q_table[state][action] = Q_table[state][action] + alpha * (expected_return - Q_table[state][action])\n",
    "            state = next_state\n",
    "\n",
    "        abs_errors.append(np.mean(np.abs(Q_table - MaxQ_table)))\n",
    "        avg_action_values.append(np.mean(Q_table[state, :]))\n",
    "\n",
    "    return Q_table, abs_errors, avg_action_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T01:11:51.961083Z",
     "start_time": "2024-05-07T01:11:51.950657Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "在考虑Roulette游戏环境和相应的奖励机制时，确定合适的先验均值（mu_prior）、先验方差（sigma_prior）、以及观测噪声（sigma_observation）对于设计有效的贝叶斯认知模型至关重要。这些参数反映了智能体对环境初步的认知和对未来观测的不确定性预期。以下是这些参数的评估方法：\n",
    "\n",
    "先验均值（mu_prior）\n",
    "先验均值代表智能体对于从环境中获取奖励的初始期望。在Roulette游戏中，考虑到多种下注方式和对应的奖励结构，合理的策略是设定一个较为保守的先验均值。考虑到大多数下注的期望值是负的（赌场优势），一个合理的先验均值可能接近于玩家下注的平均损失。例如，对于单数下注，胜率是1/37（考虑到0），奖励为35，期望收益是(1/37)35 + (36/37)(-1) ≈ -0.027。因此，一个反映整体游戏结构的先验均值可以是负数，如-0.027或类似，反映出在没有任何特定策略下的平均结果。\n",
    "\n",
    "先验方差（sigma_prior）\n",
    "先验方差反映了智能体对其奖励预期的初始不确定性。由于Roulette是一个具有高度随机性的游戏，且可能的奖励范围从-1到几十，一个较大的先验方差（例如1或更高）可能更为合适。这表示智能体对初始奖励预期的不确定性较大，准备根据接收到的奖励数据来调整其预期。\n",
    "\n",
    "观测噪声（sigma_observation）\n",
    "观测噪声表示在实际观测到的奖励值中存在的随机性或噪声水平。在Roulette游戏中，由于奖励的确定性完全依赖于随机选择的数字，观测噪声相对较小。然而，考虑到实际操作中可能存在的任何测量或估计误差，一个小的非零值（如0.1或0.01）可以提供足够的灵活性，以适应潜在的微小偏差。\n",
    "\n",
    "综合考虑以上因素，在设置Roulette环境的贝叶斯认知模型时，可以选择如下参数：\n",
    "\n",
    "mu_prior = -0.027 （或根据特定策略进行调整）\n",
    "sigma_prior = 1 （反映对奖励预期的高度不确定性）\n",
    "sigma_observation = 0.1 （反映较小的观测噪声）\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def update_bayesian_posterior(mu_prior, sigma_prior, rewards, sigma_observation=0.1):\n",
    "    epsilon = 1e-10\n",
    "    n = len(rewards)\n",
    "    if n == 0:\n",
    "        return mu_prior, sigma_prior\n",
    "    reward_sum = np.sum(rewards)\n",
    "    sigma_prior_squared = sigma_prior ** 2\n",
    "    sigma_observation_squared = sigma_observation ** 2\n",
    "    sigma_posterior_squared = 1.0 / ((1.0 / sigma_prior_squared) + (n / (sigma_observation_squared + epsilon)))\n",
    "    mu_posterior = sigma_posterior_squared * ((mu_prior / sigma_prior_squared) + (reward_sum / (sigma_observation_squared + epsilon)))\n",
    "    return mu_posterior, np.sqrt(sigma_posterior_squared)\n",
    "\n",
    "def smoothed_Q_learning_cognitive_model(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma):\n",
    "    Q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    n_table = np.zeros_like(Q_table)\n",
    "    MaxQ_table = np.full([env.observation_space.n, env.action_space.n], 0)\n",
    "    abs_errors = []\n",
    "    avg_action_values = []\n",
    "    # Initialization parameter\n",
    "    mu_prior = -0.027\n",
    "    sigma_prior = 1.0\n",
    "    sigma_observation = 0.1\n",
    "\n",
    "\n",
    "    # def softmax(Q_state, beta):\n",
    "    #     exps = np.exp(beta * (Q_state - np.max(Q_state)))\n",
    "    #     return exps / np.sum(exps)\n",
    "\n",
    "    def argmaxR(Q_state):\n",
    "        max_value_indices = np.argwhere(Q_state == np.amax(Q_state)).flatten().tolist()\n",
    "        return np.random.choice(max_value_indices)\n",
    "\n",
    "    def clipped_softmax(Q_state, beta):\n",
    "        Q_clipped = np.ones_like(Q_state) * (-np.inf)\n",
    "        top_k_indices = np.argsort(Q_state)[-3:]\n",
    "        Q_clipped[top_k_indices] = Q_state[top_k_indices] - np.max(Q_state)  # Subtract the maximum value\n",
    "        exps = np.exp(beta * Q_clipped)\n",
    "        return exps / np.sum(exps)\n",
    "\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        beta = 0.1 + 0.1 * i_episode\n",
    "        actions = []\n",
    "        epsilon = decay_epsilon(epsilon_start, epsilon_end, decay_rate, i_episode)\n",
    "        alpha = polynomial_learning_rate(base_learning_rate, learning_rate_exponent, i_episode + 1)\n",
    "        episode_rewards = []\n",
    "\n",
    "        while not done:\n",
    "            if np.random.rand() > epsilon:\n",
    "                action = argmaxR(Q_table[state])\n",
    "            else:\n",
    "                action_distribution = clipped_softmax(Q_table[state], beta)\n",
    "                action = np.random.choice(np.arange(env.action_space.n), p=action_distribution)\n",
    "\n",
    "            # action = epsilon_greedy(Q_table, state, epsilon)\n",
    "            actions.append(action)\n",
    "            n_table[state, action] += 1\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            episode_rewards.append(reward)\n",
    "            mu_prior, sigma_prior = update_bayesian_posterior(mu_prior, sigma_prior, episode_rewards, sigma_observation)\n",
    "            adjusted_Q = Q_table + mu_prior\n",
    "            action_distribution = clipped_softmax(adjusted_Q[next_state], beta)\n",
    "            expected_return = reward + gamma * np.dot(action_distribution, adjusted_Q[next_state])\n",
    "            Q_table[state][action] = Q_table[state][action] + alpha * (expected_return - Q_table[state][action])\n",
    "            state = next_state\n",
    "\n",
    "        abs_errors.append(np.mean(np.abs(Q_table - MaxQ_table)))\n",
    "        avg_action_values.append(np.mean(Q_table[state, :]))\n",
    "\n",
    "    return Q_table, abs_errors, avg_action_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T01:11:51.966468Z",
     "start_time": "2024-05-07T01:11:51.964700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rmvURpUzanA2",
    "ExecuteTime": {
     "end_time": "2024-05-07T01:11:51.970982Z",
     "start_time": "2024-05-07T01:11:51.967206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = RouletteEnv()\n",
    "\n",
    "# Set the parameters for the experiment\n",
    "num_episodes = 100000\n",
    "base_learning_rate = 0.3\n",
    "learning_rate_exponent = 0.5\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "decay_rate = 0.0001\n",
    "gamma = 0.99\n",
    "delta_start = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQj8jGB2aq7S",
    "outputId": "426622f7-1452-4d69-e6f4-afaf848ab7be",
    "ExecuteTime": {
     "end_time": "2024-05-07T01:13:11.067172Z",
     "start_time": "2024-05-07T01:11:51.973730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Learning Completed!\n"
     ]
    }
   ],
   "source": [
    "# Run the experiments\n",
    "_, Q_learning_abs_errors, Q_learning_avg_action_values = Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma)\n",
    "print(\"Q Learning Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double Q Learning Completed!\n"
     ]
    }
   ],
   "source": [
    "_, _, double_Q_learning_abs_errors, double_Q_learning_avg_action_values = double_Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma)\n",
    "print(\"Double Q Learning Completed!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T01:21:13.467976Z",
     "start_time": "2024-05-07T01:13:11.069141Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/6n9cccss24g7s1y2w9tj4kvm0000gn/T/ipykernel_93469/1703111289.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  exps = np.exp(beta * Q_normalized)\n",
      "/var/folders/ww/6n9cccss24g7s1y2w9tj4kvm0000gn/T/ipykernel_93469/1703111289.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  probs = exps / np.sum(exps)\n",
      "/Users/haydengu/anaconda3/envs/quant/lib/python3.9/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/var/folders/ww/6n9cccss24g7s1y2w9tj4kvm0000gn/T/ipykernel_93469/1703111289.py:16: RuntimeWarning: divide by zero encountered in divide\n",
      "  probs = probs / np.sum(probs)  # Ensure probabilities sum to 1\n",
      "/var/folders/ww/6n9cccss24g7s1y2w9tj4kvm0000gn/T/ipykernel_93469/1703111289.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  probs = probs / np.sum(probs)  # Ensure probabilities sum to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Smooth Q Learning Completed!\n"
     ]
    }
   ],
   "source": [
    "_, softmax_smoothed_abs_errors, softmax_smoothed_avg_action_values = smoothed_Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma, \"softmax\")\n",
    "print(\"Softmax Smooth Q Learning Completed!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T01:22:13.906038Z",
     "start_time": "2024-05-07T01:21:13.482443Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped max Smooth Q Learning Completed!\n"
     ]
    }
   ],
   "source": [
    "_, clipped_max_smoothed_abs_errors, clipped_max_smoothed_avg_action_values = smoothed_Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma, \"clipped_max\")\n",
    "print(\"Clipped max Smooth Q Learning Completed!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T01:22:17.402701Z",
     "start_time": "2024-05-07T01:22:13.906966Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped Softmax Smooth Q Learning Completed!\n"
     ]
    }
   ],
   "source": [
    "_, clipped_softmax_smoothed_abs_errors, clipped_softmax_smoothed_avg_action_values = smoothed_Q_learning(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma, \"clipped_softmax\")\n",
    "print(\"Clipped Softmax Smooth Q Learning Completed!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T01:22:20.668488Z",
     "start_time": "2024-05-07T01:22:17.403288Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m _, bayes_smoothed_abs_errors, bayes_smoothed_avg_action_values \u001B[38;5;241m=\u001B[39m \u001B[43msmoothed_Q_learning_cognitive_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_episodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbase_learning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate_exponent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon_start\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon_end\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecay_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSmooth Q Learning with Cognitive Model Completed!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[6], line 83\u001B[0m, in \u001B[0;36msmoothed_Q_learning_cognitive_model\u001B[0;34m(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma)\u001B[0m\n\u001B[1;32m     80\u001B[0m next_state, reward, done, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m     82\u001B[0m episode_rewards\u001B[38;5;241m.\u001B[39mappend(reward)\n\u001B[0;32m---> 83\u001B[0m mu_prior, sigma_prior \u001B[38;5;241m=\u001B[39m \u001B[43mupdate_bayesian_posterior\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmu_prior\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msigma_prior\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisode_rewards\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msigma_observation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m adjusted_Q \u001B[38;5;241m=\u001B[39m Q_table \u001B[38;5;241m+\u001B[39m mu_prior\n\u001B[1;32m     85\u001B[0m action_distribution \u001B[38;5;241m=\u001B[39m clipped_softmax(adjusted_Q[next_state], beta)\n",
      "Cell \u001B[0;32mIn[6], line 26\u001B[0m, in \u001B[0;36mupdate_bayesian_posterior\u001B[0;34m(mu_prior, sigma_prior, rewards, sigma_observation)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m mu_prior, sigma_prior\n\u001B[0;32m---> 26\u001B[0m reward_sum \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrewards\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m sigma_prior_squared \u001B[38;5;241m=\u001B[39m sigma_prior \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m     28\u001B[0m sigma_observation_squared \u001B[38;5;241m=\u001B[39m sigma_observation \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/quant/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2313\u001B[0m, in \u001B[0;36msum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2310\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[1;32m   2311\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[0;32m-> 2313\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapreduction\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2314\u001B[0m \u001B[43m                      \u001B[49m\u001B[43minitial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/quant/lib/python3.9/site-packages/numpy/core/fromnumeric.py:88\u001B[0m, in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     85\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     86\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m reduction(axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n\u001B[0;32m---> 88\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mufunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpasskwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "_, bayes_smoothed_abs_errors, bayes_smoothed_avg_action_values = smoothed_Q_learning_cognitive_model(env, num_episodes, base_learning_rate, learning_rate_exponent, epsilon_start, epsilon_end, decay_rate, gamma)\n",
    "print(\"Smooth Q Learning with Cognitive Model Completed!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T20:28:20.661849Z",
     "start_time": "2024-05-07T08:28:30.175797Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, figsize=(12, 16))\n",
    "# Plot the results for expected profit\n",
    "axs[0].plot(range(num_episodes), Q_learning_avg_action_values, label='Q-learning', color='blue')\n",
    "axs[0].plot(range(num_episodes), double_Q_learning_avg_action_values, label='Double Q-learning', color='orange')\n",
    "axs[0].plot(range(num_episodes), softmax_smoothed_avg_action_values, label='Softmax Smoothed Q-learning', color='green')\n",
    "axs[0].plot(range(num_episodes), clipped_max_smoothed_avg_action_values, label='Clipped max Smoothed Q-learning', color='red')\n",
    "axs[0].plot(range(num_episodes), clipped_softmax_smoothed_avg_action_values, label='Clipped Softmax Smoothed Q-learning', color='purple')\n",
    "axs[0].plot(range(num_episodes), bayes_smoothed_avg_action_values, label='Bayesian Inference Smoothed Q-learning', color='brown')\n",
    "axs[0].set_xlabel('Number of Trials')\n",
    "axs[0].set_ylabel('Expected Profit')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the results for abs_errors\n",
    "axs[1].plot(range(num_episodes), Q_learning_abs_errors, label='Q-learning', color='blue')\n",
    "axs[1].plot(range(num_episodes), double_Q_learning_abs_errors, label='Double Q-learning', color='orange')\n",
    "axs[1].plot(range(num_episodes), softmax_smoothed_abs_errors, label='Softmax Smoothed Q-learning', color='green')\n",
    "axs[1].plot(range(num_episodes), clipped_max_smoothed_abs_errors, label='Clipped max Smoothed Q-learning', color='red')\n",
    "axs[1].plot(range(num_episodes), clipped_softmax_smoothed_abs_errors, label='Clipped Softmax Smoothed Q-learning', color='purple')\n",
    "axs[1].plot(range(num_episodes), bayes_smoothed_abs_errors, label='Bayesian Inference Smoothed Q-learning', color='brown')\n",
    "axs[1].set_xlabel('Number of Episodes')\n",
    "axs[1].set_ylabel('Abs Errors')\n",
    "axs[1].legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T08:28:28.070856Z",
     "start_time": "2024-05-07T08:28:28.070374Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, figsize=(12, 16))\n",
    "# Plot the results for expected profit\n",
    "# axs[0].plot(range(num_episodes), Q_learning_avg_action_values, label='Q-learning', color='blue')\n",
    "axs[0].plot(range(num_episodes), double_Q_learning_avg_action_values, label='Double Q-learning', color='orange')\n",
    "axs[0].plot(range(num_episodes), softmax_smoothed_avg_action_values, label='Softmax Smoothed Q-learning', color='green')\n",
    "axs[0].plot(range(num_episodes), clipped_max_smoothed_avg_action_values, label='Clipped max Smoothed Q-learning', color='red')\n",
    "# axs[0].plot(range(num_episodes), clipped_softmax_smoothed_avg_action_values, label='Clipped Softmax Smoothed Q-learning', color='purple')\n",
    "axs[0].plot(range(num_episodes), bayes_smoothed_avg_action_values, label='Smoothed Q-learning with Cognitive Model', color='brown')\n",
    "axs[0].set_xlabel('Number of Trials')\n",
    "axs[0].set_ylabel('Expected Profit')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the results for abs_errors\n",
    "# axs[1].plot(range(num_episodes), Q_learning_abs_errors, label='Q-learning', color='blue')\n",
    "axs[1].plot(range(num_episodes), double_Q_learning_abs_errors, label='Double Q-learning', color='orange')\n",
    "axs[1].plot(range(num_episodes), softmax_smoothed_abs_errors, label='Softmax Smoothed Q-learning', color='green')\n",
    "axs[1].plot(range(num_episodes), clipped_max_smoothed_abs_errors, label='Clipped max Smoothed Q-learning', color='red')\n",
    "# axs[1].plot(range(num_episodes), clipped_softmax_smoothed_abs_errors, label='Clipped Softmax Smoothed Q-learning', color='purple')\n",
    "axs[1].plot(range(num_episodes), bayes_smoothed_abs_errors, label='Smoothed Q-learning with Cognitive Model', color='brown')\n",
    "axs[1].set_xlabel('Number of Episodes')\n",
    "axs[1].set_ylabel('Abs Errors')\n",
    "axs[1].legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-07T08:28:28.071285Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-07T08:28:28.072438Z"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
